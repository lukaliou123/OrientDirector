"""
Historical-basemaps æ•°æ®åŠ è½½å™¨
å®ç°GitHub Rawæ–‡ä»¶è®¿é—®ã€æ™ºèƒ½ç¼“å­˜å’Œå†å²è¾¹ç•Œæ•°æ®è·å–
"""

import aiohttp
import json
import os
from typing import Dict, List, Optional
from datetime import datetime, timedelta
import asyncio

class HistoricalDataLoader:
    """Historical-basemapsæ•°æ®åŠ è½½å™¨"""
    
    def __init__(self):
        # GitHub rawæ–‡ä»¶çš„åŸºç¡€URL
        self.github_raw_base = "https://raw.githubusercontent.com/aourednik/historical-basemaps/master/geojson"
        self.cache_dir = "backend/data/historical_cache"
        
        # å¯ç”¨çš„å†å²æ•°æ®æ–‡ä»¶æ˜ å°„ (åŸºäºHistorical-basemapsé¡¹ç›®çš„å®é™…æ–‡ä»¶)
        # æ³¨æ„ï¼šæ–‡ä»¶åä½¿ç”¨ä¸‹åˆ’çº¿ (_) è€Œä¸æ˜¯è¿å­—ç¬¦ (-)
        self.available_datasets = {
            2000: "world_2000.geojson",
            1994: "world_1994.geojson", 
            1960: "world_1960.geojson",
            1945: "world_1945.geojson",
            1938: "world_1938.geojson",
            1920: "world_1920.geojson",
            1914: "world_1914.geojson",
            1880: "world_1880.geojson",
            1815: "world_1815.geojson",
            1783: "world_1783.geojson",
            1715: "world_1715.geojson",
            1650: "world_1650.geojson",
            1530: "world_1530.geojson",
            1492: "world_1492.geojson",
            1279: "world_1279.geojson",
            800: "world_800.geojson",
            1000: "world_1000.geojson",
            400: "world_400.geojson",
            -1: "world__1.geojson"  # å…¬å…ƒå‰1å¹´ (åŒä¸‹åˆ’çº¿)
        }
        
        # ç¼“å­˜é…ç½®
        self.cache_expiry_days = 7  # ç¼“å­˜7å¤©
        
        # ç¡®ä¿ç¼“å­˜ç›®å½•å­˜åœ¨
        os.makedirs(self.cache_dir, exist_ok=True)
        
        print(f"ğŸ“š Historicalæ•°æ®åŠ è½½å™¨å·²åˆå§‹åŒ–")
        print(f"   GitHubæº: {self.github_raw_base}")
        print(f"   ç¼“å­˜ç›®å½•: {self.cache_dir}")
        print(f"   å¯ç”¨æ•°æ®é›†: {len(self.available_datasets)} ä¸ªå†å²æ—¶æœŸ")
    
    async def get_historical_data(self, year: int) -> Dict:
        """
        è·å–æŒ‡å®šå¹´ä»½çš„å†å²è¾¹ç•Œæ•°æ®
        
        Args:
            year: ç›®æ ‡å¹´ä»½
            
        Returns:
            Dict: GeoJSONæ ¼å¼çš„å†å²è¾¹ç•Œæ•°æ®
        """
        print(f"ğŸ” è¯·æ±‚å†å²æ•°æ®: {year}å¹´")
        
        # æ‰¾åˆ°æœ€æ¥è¿‘çš„å¹´ä»½æ•°æ®
        closest_year = self.find_closest_year(year)
        filename = self.available_datasets.get(closest_year)
        
        if not filename:
            raise ValueError(f"âŒ æ²¡æœ‰æ‰¾åˆ°{year}å¹´é™„è¿‘çš„å†å²æ•°æ®")
        
        print(f"ğŸ“Š æ˜ å°„åˆ°æ•°æ®æ–‡ä»¶: {filename} ({closest_year}å¹´)")
        
        # å°è¯•ä»æœ¬åœ°ç¼“å­˜åŠ è½½
        cached_data = await self.load_from_cache(filename)
        if cached_data:
            return cached_data
        
        # ä»GitHubä¸‹è½½æ•°æ®
        return await self.download_and_cache(filename, closest_year)
    
    def find_closest_year(self, target_year: int) -> int:
        """
        æ‰¾åˆ°æœ€æ¥è¿‘ç›®æ ‡å¹´ä»½çš„å¯ç”¨æ•°æ®å¹´ä»½
        
        Args:
            target_year: ç›®æ ‡å¹´ä»½
            
        Returns:
            int: æœ€æ¥è¿‘çš„å¯ç”¨å¹´ä»½
        """
        available_years = list(self.available_datasets.keys())
        
        # æ‰¾åˆ°æœ€æ¥è¿‘çš„å¹´ä»½
        closest_year = min(available_years, key=lambda x: abs(x - target_year))
        
        year_diff = abs(closest_year - target_year)
        print(f"ğŸ¯ ç›®æ ‡å¹´ä»½: {target_year}, ä½¿ç”¨æ•°æ®: {closest_year} (å·®è·: {year_diff}å¹´)")
        
        return closest_year
    
    async def download_and_cache(self, filename: str, year: int) -> Dict:
        """
        ä»GitHubä¸‹è½½GeoJSONæ–‡ä»¶å¹¶ç¼“å­˜åˆ°æœ¬åœ°
        
        Args:
            filename: GeoJSONæ–‡ä»¶å
            year: å¯¹åº”å¹´ä»½
            
        Returns:
            Dict: ä¸‹è½½çš„GeoJSONæ•°æ®
        """
        url = f"{self.github_raw_base}/{filename}"
        
        try:
            print(f"ğŸŒ æ­£åœ¨ä»GitHubä¸‹è½½å†å²æ•°æ®: {filename}")
            print(f"   URL: {url}")
            
            async with aiohttp.ClientSession(timeout=aiohttp.ClientTimeout(total=30)) as session:
                async with session.get(url) as response:
                    if response.status == 200:
                        # GitHub Rawæ–‡ä»¶è¿”å›text/plainï¼Œéœ€è¦æ‰‹åŠ¨è§£æJSON
                        text_content = await response.text(encoding='utf-8')
                        
                        try:
                            # æ‰‹åŠ¨è§£æJSONå†…å®¹
                            data = json.loads(text_content)
                        except json.JSONDecodeError as e:
                            raise ValueError(f"JSONè§£æå¤±è´¥: {e}")
                        
                        # éªŒè¯æ•°æ®å®Œæ•´æ€§
                        if not self.validate_geojson_data(data):
                            raise ValueError("ä¸‹è½½çš„æ•°æ®æ ¼å¼ä¸æ­£ç¡®")
                        
                        # ç¼“å­˜åˆ°æœ¬åœ°
                        await self.save_to_cache(filename, data)
                        
                        features_count = len(data.get('features', []))
                        print(f"âœ… å†å²æ•°æ®ä¸‹è½½æˆåŠŸ: {features_count} ä¸ªæ”¿æ²»å®ä½“")
                        
                        return data
                    else:
                        raise Exception(f"GitHubæ•°æ®ä¸‹è½½å¤±è´¥: HTTP {response.status}")
        
        except Exception as e:
            print(f"âŒ ä¸‹è½½å¤±è´¥: {e}")
            # è¿”å›é™çº§æ•°æ®
            return await self.get_fallback_data(year)
    
    async def load_from_cache(self, filename: str) -> Optional[Dict]:
        """
        ä»æœ¬åœ°ç¼“å­˜åŠ è½½æ•°æ®
        
        Args:
            filename: ç¼“å­˜æ–‡ä»¶å
            
        Returns:
            Optional[Dict]: ç¼“å­˜çš„æ•°æ®ï¼Œå¦‚æœä¸å­˜åœ¨æˆ–è¿‡æœŸåˆ™è¿”å›None
        """
        cache_path = os.path.join(self.cache_dir, filename)
        
        # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
        if not os.path.exists(cache_path):
            return None
        
        try:
            # æ£€æŸ¥ç¼“å­˜æ˜¯å¦è¿‡æœŸ
            file_modified_time = datetime.fromtimestamp(os.path.getmtime(cache_path))
            expiry_time = file_modified_time + timedelta(days=self.cache_expiry_days)
            
            if datetime.now() > expiry_time:
                print(f"â° ç¼“å­˜å·²è¿‡æœŸ: {filename} (ä¿®æ”¹æ—¶é—´: {file_modified_time})")
                return None
            
            # åŠ è½½ç¼“å­˜æ•°æ®
            with open(cache_path, 'r', encoding='utf-8') as f:
                data = json.load(f)
                
            print(f"ğŸ“ ä»ç¼“å­˜åŠ è½½å†å²æ•°æ®: {filename}")
            print(f"   ç¼“å­˜æ—¶é—´: {file_modified_time}")
            print(f"   æ•°æ®ç‰¹å¾æ•°: {len(data.get('features', []))}")
            
            return data
            
        except Exception as e:
            print(f"âš ï¸ ç¼“å­˜æ–‡ä»¶æŸåæˆ–è¯»å–å¤±è´¥: {e}")
            # åˆ é™¤æŸåçš„ç¼“å­˜æ–‡ä»¶
            try:
                os.remove(cache_path)
                print(f"ğŸ—‘ï¸ å·²åˆ é™¤æŸåçš„ç¼“å­˜æ–‡ä»¶: {filename}")
            except:
                pass
            return None
    
    async def save_to_cache(self, filename: str, data: Dict):
        """
        ä¿å­˜æ•°æ®åˆ°æœ¬åœ°ç¼“å­˜
        
        Args:
            filename: æ–‡ä»¶å
            data: è¦ç¼“å­˜çš„æ•°æ®
        """
        cache_path = os.path.join(self.cache_dir, filename)
        
        try:
            # ç¡®ä¿ç›®å½•å­˜åœ¨
            os.makedirs(self.cache_dir, exist_ok=True)
            
            # ä¿å­˜æ•°æ®
            with open(cache_path, 'w', encoding='utf-8') as f:
                json.dump(data, f, ensure_ascii=False, indent=2)
            
            # è·å–æ–‡ä»¶å¤§å°
            file_size = os.path.getsize(cache_path)
            print(f"ğŸ’¾ å†å²æ•°æ®å·²ç¼“å­˜: {cache_path}")
            print(f"   æ–‡ä»¶å¤§å°: {file_size / 1024 / 1024:.2f} MB")
            
        except Exception as e:
            print(f"âš ï¸ ç¼“å­˜ä¿å­˜å¤±è´¥: {e}")
    
    def validate_geojson_data(self, data: Dict) -> bool:
        """
        éªŒè¯GeoJSONæ•°æ®çš„å®Œæ•´æ€§
        
        Args:
            data: å¾…éªŒè¯çš„æ•°æ®
            
        Returns:
            bool: æ•°æ®æ˜¯å¦æœ‰æ•ˆ
        """
        try:
            # æ£€æŸ¥åŸºæœ¬GeoJSONç»“æ„
            if not isinstance(data, dict):
                return False
            
            if data.get('type') != 'FeatureCollection':
                return False
            
            features = data.get('features', [])
            if not isinstance(features, list) or len(features) == 0:
                return False
            
            # æ£€æŸ¥ç‰¹å¾çš„åŸºæœ¬ç»“æ„
            sample_feature = features[0]
            if not isinstance(sample_feature, dict):
                return False
            
            # æ£€æŸ¥å¿…éœ€çš„å±æ€§
            if 'geometry' not in sample_feature or 'properties' not in sample_feature:
                return False
            
            print(f"âœ… GeoJSONæ•°æ®éªŒè¯é€šè¿‡: {len(features)} ä¸ªç‰¹å¾")
            return True
            
        except Exception as e:
            print(f"âŒ GeoJSONæ•°æ®éªŒè¯å¤±è´¥: {e}")
            return False
    
    async def get_fallback_data(self, year: int) -> Dict:
        """
        å½“ä¸‹è½½å¤±è´¥æ—¶æä¾›é™çº§æ•°æ®
        
        Args:
            year: ç›®æ ‡å¹´ä»½
            
        Returns:
            Dict: ç®€å•çš„é™çº§æ•°æ®ç»“æ„
        """
        print(f"ğŸ”„ ç”Ÿæˆ{year}å¹´çš„é™çº§å†å²æ•°æ®")
        
        # åˆ›å»ºç®€å•çš„å…¨çƒé™çº§æ•°æ®
        fallback_features = []
        
        # æ ¹æ®å¹´ä»½åˆ›å»ºä¸€äº›åŸºæœ¬çš„æ”¿æ²»å®ä½“
        if year >= 1900:
            # ç°ä»£å›½å®¶
            entities = [
                {"name": "ä¸­å›½", "coords": [[100, 20], [140, 20], [140, 50], [100, 50], [100, 20]]},
                {"name": "ç¾å›½", "coords": [[-125, 25], [-65, 25], [-65, 50], [-125, 50], [-125, 25]]},
                {"name": "æ¬§ç›Ÿ", "coords": [[-10, 35], [30, 35], [30, 70], [-10, 70], [-10, 35]]}
            ]
        elif year >= 1500:
            # æ—©æœŸç°ä»£
            entities = [
                {"name": "æ˜æœ", "coords": [[100, 20], [140, 20], [140, 50], [100, 50], [100, 20]]},
                {"name": "å¥¥æ–¯æ›¼å¸å›½", "coords": [[20, 30], [50, 30], [50, 45], [20, 45], [20, 30]]},
                {"name": "è¥¿ç­ç‰™å¸å›½", "coords": [[-10, 35], [5, 35], [5, 45], [-10, 45], [-10, 35]]}
            ]
        else:
            # å¤ä»£æ–‡æ˜
            entities = [
                {"name": "æ±‰æœ", "coords": [[100, 20], [140, 20], [140, 50], [100, 50], [100, 20]]},
                {"name": "ç½—é©¬å¸å›½", "coords": [[-10, 30], [30, 30], [30, 50], [-10, 50], [-10, 30]]},
                {"name": "å¤å°åº¦", "coords": [[65, 5], [95, 5], [95, 35], [65, 35], [65, 5]]}
            ]
        
        for entity in entities:
            feature = {
                "type": "Feature",
                "properties": {
                    "NAME": entity["name"],
                    "SUBJECTO": entity["name"],
                    "PARTOF": "ä¸–ç•Œæ–‡æ˜",
                    "BORDERPRECISION": 1
                },
                "geometry": {
                    "type": "Polygon",
                    "coordinates": [entity["coords"]]
                }
            }
            fallback_features.append(feature)
        
        fallback_data = {
            "type": "FeatureCollection",
            "features": fallback_features
        }
        
        return fallback_data
    
    async def get_available_years(self) -> List[int]:
        """
        è·å–æ‰€æœ‰å¯ç”¨çš„å†å²å¹´ä»½åˆ—è¡¨
        
        Returns:
            List[int]: å¯ç”¨å¹´ä»½åˆ—è¡¨
        """
        return sorted(list(self.available_datasets.keys()), reverse=True)
    
    def get_cache_info(self) -> Dict:
        """
        è·å–ç¼“å­˜ä¿¡æ¯å’Œç»Ÿè®¡
        
        Returns:
            Dict: ç¼“å­˜ç»Ÿè®¡ä¿¡æ¯
        """
        cache_info = {
            'cache_dir': self.cache_dir,
            'cached_files': [],
            'total_cache_size': 0,
            'cache_count': 0
        }
        
        if os.path.exists(self.cache_dir):
            for filename in os.listdir(self.cache_dir):
                if filename.endswith('.geojson'):
                    file_path = os.path.join(self.cache_dir, filename)
                    file_size = os.path.getsize(file_path)
                    file_modified = datetime.fromtimestamp(os.path.getmtime(file_path))
                    
                    cache_info['cached_files'].append({
                        'filename': filename,
                        'size_mb': file_size / 1024 / 1024,
                        'modified': file_modified.isoformat(),
                        'is_expired': (datetime.now() - file_modified).days > self.cache_expiry_days
                    })
                    
                    cache_info['total_cache_size'] += file_size
                    cache_info['cache_count'] += 1
        
        cache_info['total_cache_size_mb'] = cache_info['total_cache_size'] / 1024 / 1024
        return cache_info
    
    async def clear_expired_cache(self):
        """
        æ¸…ç†è¿‡æœŸçš„ç¼“å­˜æ–‡ä»¶
        """
        if not os.path.exists(self.cache_dir):
            return
        
        expired_count = 0
        freed_size = 0
        
        for filename in os.listdir(self.cache_dir):
            if filename.endswith('.geojson'):
                file_path = os.path.join(self.cache_dir, filename)
                file_modified = datetime.fromtimestamp(os.path.getmtime(file_path))
                
                if (datetime.now() - file_modified).days > self.cache_expiry_days:
                    file_size = os.path.getsize(file_path)
                    os.remove(file_path)
                    expired_count += 1
                    freed_size += file_size
                    print(f"ğŸ—‘ï¸ åˆ é™¤è¿‡æœŸç¼“å­˜: {filename}")
        
        if expired_count > 0:
            print(f"ğŸ§¹ ç¼“å­˜æ¸…ç†å®Œæˆ: åˆ é™¤{expired_count}ä¸ªæ–‡ä»¶, é‡Šæ”¾{freed_size/1024/1024:.2f}MBç©ºé—´")
        else:
            print(f"âœ¨ ç¼“å­˜æ— éœ€æ¸…ç†")
    
    async def preload_common_datasets(self, years: List[int] = None):
        """
        é¢„åŠ è½½å¸¸ç”¨çš„å†å²æ•°æ®é›†
        
        Args:
            years: è¦é¢„åŠ è½½çš„å¹´ä»½åˆ—è¡¨ï¼Œé»˜è®¤ä¸ºå¸¸ç”¨å¹´ä»½
        """
        if years is None:
            # é»˜è®¤é¢„åŠ è½½ä¸€äº›å…³é”®å†å²æ—¶æœŸ
            years = [2000, 1945, 1914, 1880, 1650, 1492, 1000, 400]
        
        print(f"ğŸ“¥ å¼€å§‹é¢„åŠ è½½å†å²æ•°æ®é›†: {len(years)} ä¸ªæ—¶æœŸ")
        
        # å¹¶å‘ä¸‹è½½ä»¥æé«˜æ•ˆç‡
        download_tasks = []
        for year in years:
            if year in self.available_datasets:
                task = self.get_historical_data(year)
                download_tasks.append(task)
        
        if download_tasks:
            try:
                results = await asyncio.gather(*download_tasks, return_exceptions=True)
                
                success_count = sum(1 for r in results if not isinstance(r, Exception))
                print(f"âœ… é¢„åŠ è½½å®Œæˆ: {success_count}/{len(download_tasks)} ä¸ªæ•°æ®é›†æˆåŠŸ")
                
            except Exception as e:
                print(f"âš ï¸ é¢„åŠ è½½è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
    
    def get_dataset_info(self, year: int) -> Dict:
        """
        è·å–æŒ‡å®šå¹´ä»½æ•°æ®é›†çš„ä¿¡æ¯
        
        Args:
            year: å¹´ä»½
            
        Returns:
            Dict: æ•°æ®é›†ä¿¡æ¯
        """
        closest_year = self.find_closest_year(year)
        filename = self.available_datasets.get(closest_year)
        
        if not filename:
            return {'error': 'æ•°æ®é›†ä¸å­˜åœ¨'}
        
        cache_path = os.path.join(self.cache_dir, filename)
        
        info = {
            'target_year': year,
            'closest_available_year': closest_year,
            'year_difference': abs(year - closest_year),
            'filename': filename,
            'github_url': f"{self.github_raw_base}/{filename}",
            'cached': os.path.exists(cache_path)
        }
        
        if info['cached']:
            file_size = os.path.getsize(cache_path)
            file_modified = datetime.fromtimestamp(os.path.getmtime(cache_path))
            info.update({
                'cache_size_mb': file_size / 1024 / 1024,
                'cache_modified': file_modified.isoformat(),
                'cache_expired': (datetime.now() - file_modified).days > self.cache_expiry_days
            })
        
        return info


# å…¨å±€å®ä¾‹
historical_data_loader = HistoricalDataLoader()


# æµ‹è¯•å’Œè°ƒè¯•å‡½æ•°
async def test_data_loader():
    """æµ‹è¯•æ•°æ®åŠ è½½å™¨åŠŸèƒ½"""
    print("ğŸ§ª å¼€å§‹æµ‹è¯•Historicalæ•°æ®åŠ è½½å™¨...")
    
    try:
        # æµ‹è¯•å‡ ä¸ªå…³é”®å¹´ä»½
        test_years = [1600, 1945, 800, 2000]
        
        for year in test_years:
            print(f"\nğŸ“… æµ‹è¯•å¹´ä»½: {year}")
            data = await historical_data_loader.get_historical_data(year)
            
            if data and 'features' in data:
                print(f"âœ… æˆåŠŸè·å–æ•°æ®: {len(data['features'])} ä¸ªæ”¿æ²»å®ä½“")
            else:
                print(f"âŒ æ•°æ®è·å–å¤±è´¥")
    
        # æ˜¾ç¤ºç¼“å­˜ç»Ÿè®¡
        print(f"\nğŸ“Š ç¼“å­˜ç»Ÿè®¡:")
        cache_info = historical_data_loader.get_cache_info()
        print(f"   ç¼“å­˜æ–‡ä»¶æ•°: {cache_info['cache_count']}")
        print(f"   æ€»ç¼“å­˜å¤§å°: {cache_info['total_cache_size_mb']:.2f} MB")
        
    except Exception as e:
        print(f"âŒ æµ‹è¯•å¤±è´¥: {e}")


if __name__ == "__main__":
    # å¦‚æœç›´æ¥è¿è¡Œæ­¤æ–‡ä»¶ï¼Œæ‰§è¡Œæµ‹è¯•
    asyncio.run(test_data_loader())
